---
title: "The smallest PPL"
format: html
editor: visual
---

## Setup

We have a prior $\pi_0(x)$ and a likelihood $L(x) = L(y|x)$, we want to approximately sample from $\pi(x) \propto \pi_0(x) L(x)$.

Today we will consider a naive Self-Normalizing Importance Sampling estimator with target $\pi$ and proposal $\pi_0$ :

$$
\begin{align}
\hat F &= \sum_{i=1}^n \frac{w(X_i)}{\sum_j w(X_j)} f(X_i) \\
w(x) &\propto \frac{\pi(x)}{\pi_0(x)} = L(x).
\end{align}
$$

Here each $X_i$ is called a particle and $w(X_i)$ is called its weight.

The likelihood is a product of factors, $L(x) = \prod_{k=1}^K L(y_k | x)$ corresponding to $K$ observations.

## Model example

-   Imagine a bag with $K+1$ biased coins

-   Coin number $i$ in $\{0, 1, 2, ..., K\}$ has bias $p_i = i/K$

-   Example: $K+1 = 3$ coins

    -   First coin: $p_1 = 0/2 = 0$
    -   Second coin: $p_2 = 1/2$
    -   Third coin: $p_3 = 2/2 = 1$

-   Generative process:

    -   Step 1: Pick one of the $K+1$ coins from the bucket
    -   Step 2: Repeatedly flip the same coin

-   Mathematically, the model is: $$\begin{align}
    I &\sim \text{Unif}\{0, 1, 2, \dots, (K-1), K\} \\
    X_m | I &\sim \text{Bern}(I/K); m \in \{1, 2, 3\}
    \end{align}$$

-   Query: probability of a fair coin given we see three heads in a row, i.e. $P(I=1|X_1 = X_2 = X_3 = 1)$.

-   Analytic answer

    ![](decision-tree.png)

```{julia}
println("Analytical: ", (1/24) / (1/24 + 1/3))
```

## Goal

The user specifies the model with the following function:

```{julia}
function my_first_probabilistic_program(rng)
    coin_index = rand(rng, DiscreteUniform(0, 2)) 
    for i in 1:3 
        observe(1, Bernoulli(coin_index / 3))
    end
    return coin_index == 1 ? 1 : 0
end
```

and then pass that function into a function called `posterior(...)` and 
obtain a Monte Carlo approximation to the above query.


## Hints

-   Start with $X_i$
    -   It is a random variable, i.e. a map from the sample space $\Omega$ to some realization.
    -   In code: $\omega \in \Omega$ corresponds to a random number generator object, which we will denote `rng` in the code
    -   So $X_i$ will correspond to a function in the code as well, which we will denote `probabilistic_program` in the following. From the last bullet, this function takes as input a random number generator, so sampling is done via `probabilistic_program(rng)`
-   To make things simple, we will take $f(X_i)$ to simply be the value returned by `probabilistic_program`
-   To compute the weight:
    -   we will use a global variable called `current_log_likelihood` that we reset each time we are about to create a new particle
    -   each time the code in `probabilistic_program` encounters a call to `observe`, increment `current_log_likelihood`


## Julia implementation


```{julia}
using Pkg 
Pkg.activate(".")
using Distributions
using SplittableRandoms

const current_log_likelihood = Ref(0.0)

function observe(observation, distribution)
    current_log_likelihood[] += logpdf(distribution, observation)
end

function posterior(rng, probabilistic_program, n_particles)
    samples = Float64[] 
    log_weights = Float64[]

    for i in 1:n_particles 
        current_log_likelihood[] = 0.0
        push!(samples, probabilistic_program(rng))
        push!(log_weights, current_log_likelihood[])
    end

    return sum(samples .* exponentiate_normalize(log_weights))
end

### Utils

function exponentiate_normalize(vector)
    exponentiated = exp.(vector .- maximum(vector))
    return exponentiated / sum(exponentiated)
end
```

We can verify the quality of the approximation

```{julia}
rng = SplittableRandom(1)
println("Analytical: ", (1/24) / (1/24 + 1/3))
println("        MC: ", posterior(rng, my_first_probabilistic_program, 1_000_000))
```

```{r}
```
